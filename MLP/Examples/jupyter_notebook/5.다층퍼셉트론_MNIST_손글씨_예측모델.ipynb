{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다층퍼셉트론 구조\n",
    "텐서플로우로 아래의 다층퍼셉트론을 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/dropout.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/dropout.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습데이터에서 검증 데이터 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val  = x_train[50000:60000]\n",
    "x_train = x_train[0:50000]\n",
    "y_val  = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 50000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"train data has \" + str(x_train.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_train.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation data has 10000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"validation data has \" + str(x_val.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_val.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0** 부터 **255** 까지의 그레이 스케일을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# sample to show gray scale values\n",
    "print(x_train[0][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0** 부터 **9**까지의 이미지에 해당하는 숫자를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1]\n"
     ]
    }
   ],
   "source": [
    "# sample to show labels for first train data to 10th train data\n",
    "print(y_train[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터는 **10000** 개의 샘플을 가지고 있습니다.  \n",
    "모든 테스트 데이터는 **28 * 28** 의 이미지입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data has 10000 samples\n",
      "every test data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"test data has \" + str(x_test.shape[0]) + \" samples\")\n",
    "print(\"every test data is \" + str(x_test.shape[1]) \n",
    "      + \" * \" + str(x_test.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 구조 변경하기\n",
    "다층퍼셉트론의 입력 레이어에 데이터를 넣기 위해서 2d tensor (28, 28)인 데이터를,  \n",
    "1d tensor (28*28, 1)의  형태로 바꿔줍니다.  \n",
    "이 말은 행렬 형태의 데이터를 배열 형태의 데이터로 변경한다는 의미와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/reshape_mnist.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/reshape_mnist.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(50000, 784)\n",
    "x_val = x_val.reshape(10000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 정규화\n",
    "데이터 정규화는 보통 학습 시간을 단축하고, 더 나은 성능을 구하도록 도와줍니다.  \n",
    "MNIST 데이터의 모든 값은 0부터 255의 범위 안에 있으므로, 255로 값을 나눠줌으로써, 모든 값을 0부터 1 사이의 값으로 정규화합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_val /= gray_scale\n",
    "x_test /= gray_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실제값을 one hot encoding으로 변경하기\n",
    "손실 함수에서 크로스 엔트로피를 계산하기 위해, 실제값은 one hot encoding 값으로 변경합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로우 다층퍼셉트론 그래프 구현하기\n",
    "텐서플로우로 아래의 다층퍼셉트론을 구현해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/dropout.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/dropout.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 드랍 아웃 (Drop Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 하든 레이어2(h2)에 드랍아웃을 적용합니다.\n",
    "keep_prob의 값은 모델을 학습 또는 테스트할 때 결정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x):\n",
    "    # hidden layer1\n",
    "    w1 = tf.Variable(tf.random_uniform([784,256]))\n",
    "    b1 = tf.Variable(tf.zeros([256]))\n",
    "    h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "    # hidden layer2\n",
    "    w2 = tf.Variable(tf.random_uniform([256,128]))\n",
    "    b2 = tf.Variable(tf.zeros([128]))\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "    h2_drop = tf.nn.dropout(h2, keep_prob)\n",
    "    # output layer\n",
    "    w3 = tf.Variable(tf.random_uniform([128,10]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "    logits= tf.matmul(h2_drop, w3) + b3\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 조기 종료 (Early Stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매 주기(Epoch)마다 검증 데이터로 검증 정확도를 측정합니다.  \n",
    "검증 정확도가 5번 연속으로 최고 검증 정확도보다 높지 않을 때 조기 종료를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# train hyperparameters\n",
    "epoch_cnt = 300\n",
    "batch_size = 1000\n",
    "iteration = len(x_train) // batch_size\n",
    "\n",
    "earlystop_threshold = 5\n",
    "earlystop_cnt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 모델에 입력시킬 때(feed), 드랍아웃이 있을 경우, 항상 keep_prob를 설정해주셔야합니다.  \n",
    "학습 시, 10%의 드랍아웃을 하기 위해, keep_prob를 0.9로 설정합니다.  \n",
    "테스트 시, 드랍 아웃을 사용하지 않을 것이므로, keep_prob를 1.0으로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train acc: 0.21396, val acc: 0.2286\n",
      "epoch: 1, train acc: 0.4527, val acc: 0.4696\n",
      "epoch: 2, train acc: 0.49918, val acc: 0.5155\n",
      "epoch: 3, train acc: 0.55194, val acc: 0.5709\n",
      "epoch: 4, train acc: 0.61096, val acc: 0.6268\n",
      "epoch: 5, train acc: 0.66194, val acc: 0.6773\n",
      "epoch: 6, train acc: 0.7007, val acc: 0.7154\n",
      "epoch: 7, train acc: 0.73436, val acc: 0.7497\n",
      "epoch: 8, train acc: 0.76318, val acc: 0.7737\n",
      "epoch: 9, train acc: 0.7875, val acc: 0.7987\n",
      "epoch: 10, train acc: 0.80806, val acc: 0.817\n",
      "epoch: 11, train acc: 0.82498, val acc: 0.832\n",
      "epoch: 12, train acc: 0.84012, val acc: 0.8464\n",
      "epoch: 13, train acc: 0.85284, val acc: 0.8596\n",
      "epoch: 14, train acc: 0.86386, val acc: 0.8691\n",
      "epoch: 15, train acc: 0.87184, val acc: 0.8765\n",
      "epoch: 16, train acc: 0.87898, val acc: 0.8807\n",
      "epoch: 17, train acc: 0.88506, val acc: 0.8858\n",
      "epoch: 18, train acc: 0.8905, val acc: 0.891\n",
      "epoch: 19, train acc: 0.89596, val acc: 0.8941\n",
      "epoch: 20, train acc: 0.89994, val acc: 0.8969\n",
      "epoch: 21, train acc: 0.90322, val acc: 0.8999\n",
      "epoch: 22, train acc: 0.90628, val acc: 0.9021\n",
      "epoch: 23, train acc: 0.91006, val acc: 0.906\n",
      "epoch: 24, train acc: 0.9129, val acc: 0.9078\n",
      "epoch: 25, train acc: 0.91572, val acc: 0.9096\n",
      "epoch: 26, train acc: 0.91796, val acc: 0.9128\n",
      "epoch: 27, train acc: 0.92008, val acc: 0.9137\n",
      "epoch: 28, train acc: 0.92282, val acc: 0.9161\n",
      "epoch: 29, train acc: 0.92454, val acc: 0.916\n",
      "overfitting warning: 0\n",
      "epoch: 30, train acc: 0.92646, val acc: 0.9185\n",
      "epoch: 31, train acc: 0.92802, val acc: 0.9202\n",
      "epoch: 32, train acc: 0.92978, val acc: 0.9209\n",
      "epoch: 33, train acc: 0.93122, val acc: 0.9209\n",
      "epoch: 34, train acc: 0.93318, val acc: 0.9238\n",
      "epoch: 35, train acc: 0.93414, val acc: 0.9245\n",
      "epoch: 36, train acc: 0.93556, val acc: 0.9272\n",
      "epoch: 37, train acc: 0.93714, val acc: 0.9274\n",
      "epoch: 38, train acc: 0.93842, val acc: 0.9279\n",
      "epoch: 39, train acc: 0.93982, val acc: 0.9287\n",
      "epoch: 40, train acc: 0.9403, val acc: 0.93\n",
      "epoch: 41, train acc: 0.9415, val acc: 0.9306\n",
      "epoch: 42, train acc: 0.94272, val acc: 0.9321\n",
      "epoch: 43, train acc: 0.9443, val acc: 0.9322\n",
      "epoch: 44, train acc: 0.945, val acc: 0.9329\n",
      "epoch: 45, train acc: 0.94612, val acc: 0.9327\n",
      "overfitting warning: 0\n",
      "epoch: 46, train acc: 0.94652, val acc: 0.9327\n",
      "overfitting warning: 1\n",
      "epoch: 47, train acc: 0.94798, val acc: 0.935\n",
      "epoch: 48, train acc: 0.9492, val acc: 0.9356\n",
      "epoch: 49, train acc: 0.94984, val acc: 0.9357\n",
      "epoch: 50, train acc: 0.95128, val acc: 0.9372\n",
      "epoch: 51, train acc: 0.95202, val acc: 0.9376\n",
      "epoch: 52, train acc: 0.95286, val acc: 0.9382\n",
      "epoch: 53, train acc: 0.95362, val acc: 0.939\n",
      "epoch: 54, train acc: 0.95394, val acc: 0.939\n",
      "epoch: 55, train acc: 0.95504, val acc: 0.9392\n",
      "epoch: 56, train acc: 0.95506, val acc: 0.9394\n",
      "epoch: 57, train acc: 0.95542, val acc: 0.9397\n",
      "epoch: 58, train acc: 0.9565, val acc: 0.9402\n",
      "epoch: 59, train acc: 0.95784, val acc: 0.9421\n",
      "epoch: 60, train acc: 0.95812, val acc: 0.9409\n",
      "overfitting warning: 0\n",
      "epoch: 61, train acc: 0.95868, val acc: 0.941\n",
      "overfitting warning: 1\n",
      "epoch: 62, train acc: 0.95926, val acc: 0.9425\n",
      "epoch: 63, train acc: 0.96082, val acc: 0.9422\n",
      "overfitting warning: 0\n",
      "epoch: 64, train acc: 0.96058, val acc: 0.9427\n",
      "epoch: 65, train acc: 0.96174, val acc: 0.9452\n",
      "epoch: 66, train acc: 0.96208, val acc: 0.9436\n",
      "overfitting warning: 0\n",
      "epoch: 67, train acc: 0.96204, val acc: 0.9441\n",
      "epoch: 68, train acc: 0.96242, val acc: 0.9445\n",
      "overfitting warning: 0\n",
      "epoch: 69, train acc: 0.96316, val acc: 0.9451\n",
      "overfitting warning: 1\n",
      "epoch: 70, train acc: 0.96286, val acc: 0.9437\n",
      "epoch: 71, train acc: 0.9641, val acc: 0.9455\n",
      "epoch: 72, train acc: 0.965, val acc: 0.9453\n",
      "overfitting warning: 0\n",
      "epoch: 73, train acc: 0.96514, val acc: 0.9452\n",
      "overfitting warning: 1\n",
      "epoch: 74, train acc: 0.96546, val acc: 0.9458\n",
      "epoch: 75, train acc: 0.96588, val acc: 0.9467\n",
      "epoch: 76, train acc: 0.96684, val acc: 0.9466\n",
      "overfitting warning: 0\n",
      "epoch: 77, train acc: 0.96678, val acc: 0.9454\n",
      "epoch: 78, train acc: 0.9668, val acc: 0.946\n",
      "overfitting warning: 0\n",
      "epoch: 79, train acc: 0.96732, val acc: 0.947\n",
      "epoch: 80, train acc: 0.96812, val acc: 0.9468\n",
      "overfitting warning: 0\n",
      "epoch: 81, train acc: 0.96828, val acc: 0.9477\n",
      "epoch: 82, train acc: 0.9694, val acc: 0.9481\n",
      "epoch: 83, train acc: 0.9688, val acc: 0.9466\n",
      "epoch: 84, train acc: 0.96818, val acc: 0.9457\n",
      "epoch: 85, train acc: 0.9701, val acc: 0.9472\n",
      "overfitting warning: 0\n",
      "epoch: 86, train acc: 0.97042, val acc: 0.9469\n",
      "overfitting warning: 1\n",
      "epoch: 87, train acc: 0.97086, val acc: 0.9483\n",
      "epoch: 88, train acc: 0.97064, val acc: 0.9488\n",
      "epoch: 89, train acc: 0.9717, val acc: 0.9483\n",
      "overfitting warning: 0\n",
      "epoch: 90, train acc: 0.97106, val acc: 0.9493\n",
      "epoch: 91, train acc: 0.97278, val acc: 0.9499\n",
      "epoch: 92, train acc: 0.97052, val acc: 0.9485\n",
      "epoch: 93, train acc: 0.97206, val acc: 0.949\n",
      "overfitting warning: 0\n",
      "epoch: 94, train acc: 0.97084, val acc: 0.9471\n",
      "epoch: 95, train acc: 0.9724, val acc: 0.9492\n",
      "overfitting warning: 0\n",
      "epoch: 96, train acc: 0.97282, val acc: 0.9497\n",
      "overfitting warning: 1\n",
      "epoch: 97, train acc: 0.9724, val acc: 0.9503\n",
      "epoch: 98, train acc: 0.97332, val acc: 0.9503\n",
      "epoch: 99, train acc: 0.97294, val acc: 0.9501\n",
      "epoch: 100, train acc: 0.97246, val acc: 0.9494\n",
      "epoch: 101, train acc: 0.97358, val acc: 0.9493\n",
      "overfitting warning: 0\n",
      "epoch: 102, train acc: 0.97136, val acc: 0.9474\n",
      "epoch: 103, train acc: 0.97024, val acc: 0.9475\n",
      "epoch: 104, train acc: 0.9724, val acc: 0.948\n",
      "overfitting warning: 0\n",
      "epoch: 105, train acc: 0.97264, val acc: 0.948\n",
      "overfitting warning: 1\n",
      "epoch: 106, train acc: 0.9735, val acc: 0.948\n",
      "overfitting warning: 2\n",
      "epoch: 107, train acc: 0.97448, val acc: 0.9488\n",
      "overfitting warning: 3\n",
      "epoch: 108, train acc: 0.97348, val acc: 0.9483\n",
      "epoch: 109, train acc: 0.97176, val acc: 0.947\n",
      "epoch: 110, train acc: 0.97402, val acc: 0.9487\n",
      "overfitting warning: 0\n",
      "epoch: 111, train acc: 0.9728, val acc: 0.9478\n",
      "epoch: 112, train acc: 0.97392, val acc: 0.9488\n",
      "overfitting warning: 0\n",
      "epoch: 113, train acc: 0.96948, val acc: 0.9446\n",
      "epoch: 114, train acc: 0.97106, val acc: 0.9477\n",
      "overfitting warning: 0\n",
      "epoch: 115, train acc: 0.9692, val acc: 0.9459\n",
      "epoch: 116, train acc: 0.97228, val acc: 0.9464\n",
      "overfitting warning: 0\n",
      "epoch: 117, train acc: 0.97618, val acc: 0.9505\n",
      "epoch: 118, train acc: 0.97656, val acc: 0.9506\n",
      "epoch: 119, train acc: 0.97696, val acc: 0.9505\n",
      "overfitting warning: 0\n",
      "epoch: 120, train acc: 0.97894, val acc: 0.9502\n",
      "overfitting warning: 1\n",
      "epoch: 121, train acc: 0.97946, val acc: 0.9509\n",
      "epoch: 122, train acc: 0.97802, val acc: 0.948\n",
      "epoch: 123, train acc: 0.97754, val acc: 0.9517\n",
      "epoch: 124, train acc: 0.97892, val acc: 0.9541\n",
      "epoch: 125, train acc: 0.97904, val acc: 0.9531\n",
      "overfitting warning: 0\n",
      "epoch: 126, train acc: 0.97836, val acc: 0.9513\n",
      "epoch: 127, train acc: 0.97964, val acc: 0.9533\n",
      "overfitting warning: 0\n",
      "epoch: 128, train acc: 0.98012, val acc: 0.9524\n",
      "overfitting warning: 1\n",
      "epoch: 129, train acc: 0.9796, val acc: 0.9531\n",
      "epoch: 130, train acc: 0.9781, val acc: 0.9508\n",
      "epoch: 131, train acc: 0.9772, val acc: 0.9498\n",
      "epoch: 132, train acc: 0.98106, val acc: 0.9526\n",
      "overfitting warning: 0\n",
      "epoch: 133, train acc: 0.97966, val acc: 0.9535\n",
      "epoch: 134, train acc: 0.97906, val acc: 0.9508\n",
      "epoch: 135, train acc: 0.97924, val acc: 0.9523\n",
      "overfitting warning: 0\n",
      "epoch: 136, train acc: 0.97946, val acc: 0.9512\n",
      "overfitting warning: 1\n",
      "epoch: 137, train acc: 0.97764, val acc: 0.9497\n",
      "epoch: 138, train acc: 0.98002, val acc: 0.9498\n",
      "overfitting warning: 0\n",
      "epoch: 139, train acc: 0.98006, val acc: 0.9525\n",
      "overfitting warning: 1\n",
      "epoch: 140, train acc: 0.97842, val acc: 0.9517\n",
      "epoch: 141, train acc: 0.9756, val acc: 0.9501\n",
      "epoch: 142, train acc: 0.98032, val acc: 0.9519\n",
      "overfitting warning: 0\n",
      "epoch: 143, train acc: 0.97812, val acc: 0.9514\n",
      "epoch: 144, train acc: 0.97928, val acc: 0.9524\n",
      "overfitting warning: 0\n",
      "epoch: 145, train acc: 0.97816, val acc: 0.9487\n",
      "epoch: 146, train acc: 0.97918, val acc: 0.9502\n",
      "overfitting warning: 0\n",
      "epoch: 147, train acc: 0.97952, val acc: 0.9504\n",
      "overfitting warning: 1\n",
      "epoch: 148, train acc: 0.9815, val acc: 0.9504\n",
      "overfitting warning: 2\n",
      "epoch: 149, train acc: 0.98072, val acc: 0.9509\n",
      "epoch: 150, train acc: 0.97946, val acc: 0.952\n",
      "epoch: 151, train acc: 0.978, val acc: 0.95\n",
      "epoch: 152, train acc: 0.98016, val acc: 0.9521\n",
      "overfitting warning: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 153, train acc: 0.98082, val acc: 0.9521\n",
      "overfitting warning: 1\n",
      "epoch: 154, train acc: 0.98348, val acc: 0.9553\n",
      "epoch: 155, train acc: 0.97986, val acc: 0.951\n",
      "epoch: 156, train acc: 0.98, val acc: 0.9528\n",
      "overfitting warning: 0\n",
      "epoch: 157, train acc: 0.9822, val acc: 0.9549\n",
      "overfitting warning: 1\n",
      "epoch: 158, train acc: 0.98234, val acc: 0.9542\n",
      "overfitting warning: 2\n",
      "epoch: 159, train acc: 0.98194, val acc: 0.9533\n",
      "epoch: 160, train acc: 0.9835, val acc: 0.956\n",
      "epoch: 161, train acc: 0.98424, val acc: 0.9544\n",
      "overfitting warning: 0\n",
      "epoch: 162, train acc: 0.986, val acc: 0.9559\n",
      "overfitting warning: 1\n",
      "epoch: 163, train acc: 0.98802, val acc: 0.9565\n",
      "epoch: 164, train acc: 0.98674, val acc: 0.9554\n",
      "epoch: 165, train acc: 0.9863, val acc: 0.9556\n",
      "epoch: 166, train acc: 0.98622, val acc: 0.9545\n",
      "epoch: 167, train acc: 0.98578, val acc: 0.9547\n",
      "epoch: 168, train acc: 0.98722, val acc: 0.9556\n",
      "overfitting warning: 0\n",
      "epoch: 169, train acc: 0.98764, val acc: 0.9557\n",
      "overfitting warning: 1\n",
      "epoch: 170, train acc: 0.98802, val acc: 0.9562\n",
      "overfitting warning: 2\n",
      "epoch: 171, train acc: 0.98692, val acc: 0.9548\n",
      "epoch: 172, train acc: 0.9878, val acc: 0.9575\n",
      "epoch: 173, train acc: 0.98712, val acc: 0.9566\n",
      "epoch: 174, train acc: 0.98942, val acc: 0.9576\n",
      "epoch: 175, train acc: 0.98828, val acc: 0.9558\n",
      "epoch: 176, train acc: 0.9868, val acc: 0.9555\n",
      "epoch: 177, train acc: 0.98836, val acc: 0.9564\n",
      "overfitting warning: 0\n",
      "epoch: 178, train acc: 0.98794, val acc: 0.9558\n",
      "epoch: 179, train acc: 0.98686, val acc: 0.9534\n",
      "epoch: 180, train acc: 0.98652, val acc: 0.9535\n",
      "epoch: 181, train acc: 0.98634, val acc: 0.953\n",
      "epoch: 182, train acc: 0.98512, val acc: 0.952\n",
      "epoch: 183, train acc: 0.98472, val acc: 0.9527\n",
      "epoch: 184, train acc: 0.98724, val acc: 0.956\n",
      "overfitting warning: 0\n",
      "epoch: 185, train acc: 0.98438, val acc: 0.9519\n",
      "epoch: 186, train acc: 0.98766, val acc: 0.9558\n",
      "overfitting warning: 0\n",
      "epoch: 187, train acc: 0.9878, val acc: 0.9545\n",
      "overfitting warning: 1\n",
      "epoch: 188, train acc: 0.98904, val acc: 0.9559\n",
      "overfitting warning: 2\n",
      "epoch: 189, train acc: 0.98714, val acc: 0.9525\n",
      "epoch: 190, train acc: 0.9875, val acc: 0.9539\n",
      "overfitting warning: 0\n",
      "epoch: 191, train acc: 0.98688, val acc: 0.9534\n",
      "epoch: 192, train acc: 0.98634, val acc: 0.9527\n",
      "epoch: 193, train acc: 0.98602, val acc: 0.9526\n",
      "epoch: 194, train acc: 0.98688, val acc: 0.9545\n",
      "overfitting warning: 0\n",
      "epoch: 195, train acc: 0.98702, val acc: 0.9532\n",
      "overfitting warning: 1\n",
      "epoch: 196, train acc: 0.98906, val acc: 0.9569\n",
      "overfitting warning: 2\n",
      "epoch: 197, train acc: 0.98748, val acc: 0.9535\n",
      "epoch: 198, train acc: 0.98868, val acc: 0.954\n",
      "overfitting warning: 0\n",
      "epoch: 199, train acc: 0.98726, val acc: 0.9539\n",
      "epoch: 200, train acc: 0.98832, val acc: 0.9539\n",
      "overfitting warning: 0\n",
      "epoch: 201, train acc: 0.98754, val acc: 0.9532\n",
      "epoch: 202, train acc: 0.99096, val acc: 0.9557\n",
      "overfitting warning: 0\n",
      "epoch: 203, train acc: 0.99036, val acc: 0.9559\n",
      "overfitting warning: 1\n",
      "epoch: 204, train acc: 0.98918, val acc: 0.9553\n",
      "epoch: 205, train acc: 0.99072, val acc: 0.9573\n",
      "overfitting warning: 0\n",
      "epoch: 206, train acc: 0.98754, val acc: 0.9546\n",
      "epoch: 207, train acc: 0.9901, val acc: 0.9562\n",
      "overfitting warning: 0\n",
      "epoch: 208, train acc: 0.98998, val acc: 0.9563\n",
      "epoch: 209, train acc: 0.98992, val acc: 0.9566\n",
      "epoch: 210, train acc: 0.98682, val acc: 0.9555\n",
      "epoch: 211, train acc: 0.99038, val acc: 0.959\n",
      "epoch: 212, train acc: 0.99066, val acc: 0.9581\n",
      "overfitting warning: 0\n",
      "epoch: 213, train acc: 0.9873, val acc: 0.954\n",
      "epoch: 214, train acc: 0.99002, val acc: 0.9554\n",
      "overfitting warning: 0\n",
      "epoch: 215, train acc: 0.9885, val acc: 0.9555\n",
      "epoch: 216, train acc: 0.9915, val acc: 0.9565\n",
      "overfitting warning: 0\n",
      "epoch: 217, train acc: 0.99158, val acc: 0.9557\n",
      "overfitting warning: 1\n",
      "epoch: 218, train acc: 0.9928, val acc: 0.9565\n",
      "overfitting warning: 2\n",
      "epoch: 219, train acc: 0.99116, val acc: 0.9569\n",
      "overfitting warning: 3\n",
      "epoch: 220, train acc: 0.99202, val acc: 0.9581\n",
      "overfitting warning: 4\n",
      "epoch: 221, train acc: 0.98872, val acc: 0.9561\n",
      "epoch: 222, train acc: 0.99134, val acc: 0.9595\n",
      "epoch: 223, train acc: 0.99094, val acc: 0.9569\n",
      "overfitting warning: 0\n",
      "epoch: 224, train acc: 0.9906, val acc: 0.9552\n",
      "overfitting warning: 1\n",
      "epoch: 225, train acc: 0.99142, val acc: 0.9559\n",
      "overfitting warning: 2\n",
      "epoch: 226, train acc: 0.99158, val acc: 0.9555\n",
      "overfitting warning: 3\n",
      "epoch: 227, train acc: 0.99272, val acc: 0.9562\n",
      "overfitting warning: 4\n",
      "epoch: 228, train acc: 0.9916, val acc: 0.9577\n",
      "early stopped on 228\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    prev_train_acc = 0.0\n",
    "    max_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.\n",
    "        start = 0; end = batch_size\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            _, loss = sess.run([train_op, loss_op], \n",
    "                               feed_dict={x: x_train[start: end], \n",
    "                                          y: y_train[start: end], \n",
    "                                          keep_prob: 0.9})\n",
    "            start += batch_size; end += batch_size\n",
    "            # Compute train average loss\n",
    "            avg_loss += loss / iteration\n",
    "            \n",
    "        # Validate model\n",
    "        preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        # train accuracy\n",
    "        cur_train_acc = accuracy.eval({x: x_train, y: y_train,keep_prob: 1.0})\n",
    "        # validation accuarcy\n",
    "        cur_val_acc = accuracy.eval({x: x_val, y: y_val, keep_prob: 1.0})\n",
    "        # validation loss\n",
    "        cur_val_loss = loss_op.eval({x: x_val, y: y_val,keep_prob: 1.0})\n",
    "        \n",
    "        print(\"epoch: \"+str(epoch)+\n",
    "              \", train acc: \" + str(cur_train_acc) +\n",
    "              \", val acc: \" + str(cur_val_acc) )\n",
    "              #', train loss: '+str(avg_loss)+\n",
    "              #', val loss: '+str(cur_val_loss))\n",
    "        \n",
    "        if cur_val_acc < max_val_acc:\n",
    "            if cur_train_acc > prev_train_acc or cur_train_acc > 0.99:\n",
    "                if earlystop_cnt == earlystop_threshold:\n",
    "                    print(\"early stopped on \"+str(epoch))\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"overfitting warning: \"+str(earlystop_cnt))\n",
    "                    earlystop_cnt += 1\n",
    "            else:\n",
    "                earlystop_cnt = 0\n",
    "        else:\n",
    "            earlystop_cnt = 0\n",
    "            max_val_acc = cur_val_acc\n",
    "            # Save the variables to file.\n",
    "            save_path = saver.save(sess, \"model/model.ckpt\")\n",
    "        prev_train_acc = cur_train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트\n",
    "검증 정확도가 가장 높은 모델을 대상으로 테스트를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
      "[Test Accuracy] : 0.9548\n"
     ]
    }
   ],
   "source": [
    "# Start testing\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"model/model.ckpt\")\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"[Test Accuracy] :\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
